{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20.02.2025 - Tiefer eintauchen in Textanalyse, plus Spracherkennung \n",
    "---\n",
    "Zunächst schauen wir und die Sentiment Textanalyse, mit der wir letztes mal begonnen haben, genauer an. Welche Idee steckt hinter dem genutzten Modell? Welche anderen Möglichkeiten bietet die transformers Bibliothek? Danach wechseln wir von der Textform unserer Eingabedaten zum Audioformat: Wir erarbeiten, wie online und offline Spracherkennung in Python umgesetzt werden kann.\n",
    "\n",
    "* Zur Bearbeitung der Aufgaben können Sie benötigte Informationen zu Python-Befehlen und zu KI relevanten Bibliotheken (numpy, scikit, pandas) aus allen verfügbaren Quellen beziehen. Die meisten findet man natürlich über eine Suche im Internet, oder durch die Nutzung von KI chat-Systemen selbst.\n",
    "Ein gutes Tutorial für den Start findet sich  z.B. hier: https://www.python-kurs.eu/numerisches_programmieren_in_Python.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase I: Zusatzinformationen zur Textanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die **Sentiment-Analyse** (auch Meinungs- oder Stimmungsanalyse genannt) ist ein wichtiges Werkzeug in der **Textanalyse**. Sie dient dazu, die emotionale Haltung eines Textes automatisch zu bestimmen – ob er **positiv** oder **negativ** ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben bereits mittels **TfidfVectorizer** genutzt, um Textdaten in numerische Werte umzuwandeln, um sie für maschinelles Lernen nutzbar zu machen. Es analysiert die Häufigkeit von Wörtern in einem Text (*Term Frequency*, *TF*) und berücksichtigt, wie wichtig diese Wörter im Vergleich zu anderen Texten sind (*Inverse Document Frequency*, *IDF*).\n",
    "Die resultierenden Ähnlichkeitsvektoren für Texte wurden dann mittels Kosinus-Ähnlichkeit verglichen.\n",
    "\n",
    "Ein \"tieferer\" KI-Ansatz, um die Meinung eines oder mehrerer Texte automatisch zu bestimmen, ist die Sentiment-Analyse basierend auf dem \"BERT\"-Modell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiederholung der Aufgabe: Sentiment-Analyse mit einem vortrainierten BERT-Modell\n",
    "\n",
    "In dieser Aufgabe führen wir eine Sentiment-Analyse auf mehreren deutschen Texten durch, um mittels KI zu analysieren, ob die Texte eine positive oder negative Aussage haben.\n",
    "\n",
    "Installieren Sie zuerst das transformers und das torch Paket: \n",
    "```python\n",
    "pip install transformers torch\n",
    "```\n",
    "\n",
    "Nutze Sie in Ihrem Programm aus der `transformers` Bibliothek die `pipeline` Funktionalität. Sie bietet Zugang zu einem Modell, das auf Sentiment-Analyse spezialisiert ist.\n",
    "Informieren Sie sich, wie ein Modell für deutsche Texte genutzt werden kann.\n",
    "\n",
    "Analysieren Sie die folgenden Textbeispiele:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts = [\n",
    "    \"Einfach einzurichten, vielseitig nutzbar, starke Leistungsübertragung, wurde mir empfohlen und kann ich absolut weiter empfehlen.\",\n",
    "    \"Es gibt kaum besseres. Wenn man weiß, wie man ein WLAN-Mesh mit AVM Geräten einrichtet, der sollte hier keine Probleme haben.\",\n",
    "    \"Ich kann dieses Produkt nicht empfehlen. Wer ein stabiles und benutzerfreundliches Mesh-System sucht, sollte sich nach Alternativen umsehen.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Das Produkt ist schlecht\n",
      "Sentiment: negative (Confidence: 0.9958)\n",
      "\n",
      "Text: Einfach einzurichten, vielseitig nutzbar, starke Leistungsübertragung, wurde mir empfohlen und kann ich absolut weiter empfehlen.\n",
      "Sentiment: positive (Confidence: 0.9995)\n",
      "\n",
      "Text: Es gibt kaum besseres. Wenn man weiß, wie man ein WLAN-Mesh mit AVM Geräten einrichtet, der sollte hier keine Probleme haben.\n",
      "Sentiment: positive (Confidence: 0.9949)\n",
      "\n",
      "Text: Ich kann dieses Produkt nicht empfehlen. Wer ein stabiles und benutzerfreundliches Mesh-System sucht, sollte sich nach Alternativen umsehen.\n",
      "Sentiment: negative (Confidence: 0.9978)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Von Bibliothek transfomerts das tool pipeline importieren\n",
    "from transformers import pipeline\n",
    "\n",
    "# Sentiment-Analyse-Pipeline mit deutschem Modell laden\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"oliverguhr/german-sentiment-bert\")\n",
    "\n",
    "# Beispieltexte zur Analyse (Deutsch)\n",
    "texts = [\n",
    "    \"Das Produkt ist schlecht\",\n",
    "    \"Einfach einzurichten, vielseitig nutzbar, starke Leistungsübertragung, wurde mir empfohlen und kann ich absolut weiter empfehlen.\",\n",
    "    \"Es gibt kaum besseres. Wenn man weiß, wie man ein WLAN-Mesh mit AVM Geräten einrichtet, der sollte hier keine Probleme haben.\",\n",
    "    \"Ich kann dieses Produkt nicht empfehlen. Wer ein stabiles und benutzerfreundliches Mesh-System sucht, sollte sich nach Alternativen umsehen.\"\n",
    "]\n",
    "\n",
    "# Analyse der Texte\n",
    "results = sentiment_pipeline(texts)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for i in range(len(texts)):\n",
    "    text = texts[i]\n",
    "    result = results[i]\n",
    "    print(f\"Text: {text}\\nSentiment: {result['label']} (Confidence: {result['score']:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammengefasst:\n",
    "\n",
    "- **NLP (Natural Language Processing)** ist der Bereich der Künstlichen Intelligenz, der sich mit der Verarbeitung von Sprache beschäftigt. Es geht darum, Computern zu ermöglichen, menschliche Sprache zu verstehen, zu interpretieren und darauf zu reagieren.\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)** ist ein Modell, das für verschiedene NLP-Aufgaben entwickelt wurde. Es versteht den Kontext eines Textes sowohl aus der linken als auch aus der rechten Seite eines Wortes, was zu einer besseren Textverarbeitung führt.\n",
    "\n",
    "- **Transformers** ist eine Python-Bibliothek, die es ermöglicht, BERT und viele andere vortrainierte Modelle für NLP-Aufgaben einfach zu verwenden. Die Bibliothek wurde von Hugging Face entwickelt und bietet eine Vielzahl von Tools zur Bearbeitung von Textdaten.\n",
    "\n",
    "- **Pipeline** ist eine benutzerfreundliche Schnittstelle innerhalb der Transformers-Bibliothek, die vortrainierte Modelle für eine Vielzahl von NLP-Aufgaben (wie Textklassifikation, Named Entity Recognition und Frage-Antwort-Systeme) zur Verfügung stellt. Mit wenigen Zeilen Code kann man so komplexe Aufgaben lösen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die BERT Sentiment Analyse\n",
    "\n",
    "**BERT** ,einschließlich `oliverguhr/german-sentiment-bert`, ist ein Deep Learning-Modell. Es nutzt die eine Architektur mit Selbstaufmerksamkeit, um die Beziehungen zwischen Wörtern und deren Kontext zu erfassen und zu verstehen. Das Modell wurde durch Vortraining und anschließendem Fine-Tuning auf die Sentiment-Analyse angepasst.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Funktionsweise\n",
    "\n",
    "| Punkt                                | Beschreibung                                                                                     |\n",
    "|--------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **1. Vortraining**                   | Das Modell wird auf einem großen Korpus von Textdaten vortrainiert, um Sprachverständnis zu erlangen. |\n",
    "| 1.a. Masked Language Modeling (MLM) | Teilweise maskierte Wörter im Text werden vom Modell vorhergesagt, um den Kontext zu verstehen. Beispiel: \"Der Hund läuft im [MASK].\" |\n",
    "| 1.b. Next Sentence Prediction (NSP)  | Das Modell sagt voraus, ob der zweite Satz sinnvoll auf den ersten folgt. Beispiel: \"Der Hund läuft im Park.\" -> \"Er sieht viele Vögel.\" |\n",
    "| **2. Feinabstimmung**                | Das Modell wird mit gelabelten Daten (positiv, neutral, negativ) trainiert, um Sentiment-Klassen vorherzusagen. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. Vortraining:\n",
    "Das Modell wird zuerst auf einem riesigen Korpus von Textdaten (z. B. Wikipedia) vortrainiert, um allgemeines Sprachverständnis zu erlangen. Dabei lernt es, wie Wörter im Kontext miteinander verbunden sind, ohne dass es explizite Labels (wie Sentiment) benötigt.\n",
    "\n",
    "##### 1.a. Masked Language Modeling (MLM):\n",
    "Bei MLM wird ein Teil der Wörter im Text \"maskiert\" (also durch ein spezielles Token ersetzt, z. B. `[MASK]`), und das Modell muss vorhersagen, welches Wort an dieser Stelle steht.\n",
    "\n",
    "Beispiel:\n",
    "- **Originaltext**: \"Der Hund läuft im Park.\"\n",
    "- **Maskierter Text**: \"Der Hund läuft im [MASK].\"\n",
    "- Das Modell muss vorhersagen, dass das fehlende Wort \"Park\" ist.\n",
    "\n",
    "Diese Methode zwingt das Modell, den Kontext auf beiden Seiten eines Wortes zu verstehen, um es richtig zu erraten. So lernt das Modell, die Beziehungen zwischen den Wörtern und deren Bedeutungen zu erkennen, ohne dass es explizite Anweisungen über die Bedeutung der Wörter erhält.\n",
    "\n",
    "##### 1.b. Next Sentence Prediction (NSP):\n",
    "\n",
    "Bei NSP wird das Modell mit zwei Sätzen konfrontiert und muss vorhersagen, ob der zweite Satz inhaltlich auf den ersten folgt oder nicht.\n",
    "\n",
    "Beispiel:\n",
    "- **Satz 1**: „Der Hund läuft im Park.“\n",
    "- **Satz 2**: „Er sieht viele Vögel.“\n",
    "- Die Frage an das Modell wäre: „Folgt Satz 2 sinnvoll auf Satz 1?“ (Ja, in diesem Fall.)\n",
    "\n",
    "Dadurch lernt BERT, Beziehungen zwischen Sätzen zu erkennen und zu verstehen, wie ein Text zusammenhängend strukturiert ist.\n",
    "\n",
    "\n",
    "##### 2. Feinabstimmung:\n",
    "Danach wird das Modell auf spezifische Aufgaben wie Sentiment-Analyse angepasst. Beim Fine-Tuning wird BERT mit gelabelten Daten (positiv, negativ) trainiert, um die richtige Sentiment-Klasse für neue Texte vorherzusagen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Aufgabe: Die BERT-Sentiment-Analyse genauer kennenlernen\n",
    "\n",
    "Wenden Sie die BERT-Sentiment-Analyse aus obiger Aufgabe auf verschiedene, selbstgewählte Texte an. Beobachten Sie die Qualität der automatischen Einstufung in positiv und negativ. \n",
    "\n",
    "Mögliche Fragestellungen:\n",
    "- Ab wann kippt eine Bewertung von positiv nach negativ (oder andersherum).\n",
    "- Mischung positiver und negativer Aussagen in einem Text.\n",
    "- Versuchen Sie auch einmal, die BERT-Sentiment-Analyse \"auszutricksen\", indem Sie komplexe Sätze bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aufgabe: Die Transformers Pipeline zur Textvervollständigung\n",
    "\n",
    "Die *transformers* Bibliothek und deren *pipeline* Schnittstelle bietet mehr Features zur Textanalyse als nur die Sentiment Analyse.\n",
    "Eine weitere Möglichkeit ist die *Text-Vervollständigung*. Diese Funktion wird verwendet, um fehlende Wörter in einem Text zu ergänzen.\n",
    "\n",
    "Versuchen Sie zur Lösung der Aufgabe folgendes (zunächst ohne online nach Informationen zu suchen):\n",
    "Ändern Sie das Programm zur Sentiment Analyse von oben so ab, dass die *fill-mask* Funktion benutzt wird. Ein passendes deutschsprachiges Modell ist *dbmdz/bert-base-german-cased*.\n",
    "Das Resultat der pipeline mit der Fill-Mask Funktion ist wieder eine Liste von Dictionaries. Für jeden Textvorschlag ein Dictionary. Jedes Dictionary enthält die Schlüssel \"sequence\" und \"score\". Die Werte hinter diesen Schlüsseln entsprechen der Satzvervollständigung und der zugehörigen Wahrscheinlichkeit.\n",
    "\n",
    "Die Fill-Mask Funktin sagt mit Hilfe des Modells vorher, welches Wort am besten an die Stelle eines Platzhalters [MASK] in einem Text passt. Wenden Sie es an auf den Satz:\n",
    "\n",
    "\"Er [MASK] vom Fahrrad und brach sich den Arm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png) \n",
    "Bild von Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Von Bibliothek transfomerts das tool pipeline importieren\n",
    "\n",
    "\n",
    "# Pipeline für Masked Language Modeling (fill-mask) mit deutschem BERT laden\n",
    "\n",
    "\n",
    "# Beispieltext mit einer Lücke ([MASK])\n",
    "\n",
    "\n",
    "# Modell ausführen\n",
    "\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase II - Spracherkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spracherkennung ist eine der zentralen Anwenudngen in der Künstlichen Intelligenz, denn sie ermöglicht es, die natürliche, gesprochene Sprache zu verstehen und zu verarbeiten. Sobald gesprochene Sprache in Text umgewandelt wurde, können alle Technologien zur Textverarbeitung angewandt werden. Spracherkennung bildet die Grundlage für digitale Assistenten wie Siri oder Alexa, Transkriptionssoftware und sprachgesteuerte Steuerungssysteme.  \n",
    "Durch Fortschritte in Deep Learning und neuronalen Netzen hat sich die Genauigkeit der Spracherkennung erheblich verbessert. Moderne Systeme nutzen riesige Sprachmodelle und selbstlernende Algorithmen, um Dialekte, Akzente und sogar Emotionen in der Sprache besser zu erfassen.\n",
    "\n",
    "Dank leistungsstarker Bibliotheken wie SpeechRecognition, whisper und vosk kann man in Python mit wenigen Zeilen Code gesprochene Sprache in Text umwandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Aufgabe\n",
    "\n",
    "Wir starten mit einer Spracherkennungsengine von Google.\n",
    "Installieren Sie dazu das Python Paket `SpeechRecognition`.\n",
    "<!-- <span style=\"font-family: monospace; color:white;\"> SpeechRecognition </span>. -->\n",
    "\n",
    "Informieren Sie Sich über das Modul SpeechRecognition und seine Arbeitsweise. \n",
    "Schreiben Sie ein Python Programm, das mit dieser Bibliothek eine Audio-Datei mit deutscher Sprache in Text umwandelt. Der erkannte Text soll ausgegeben werden.\n",
    "Es gibt unterschiedliche ErkennungsModelle (\"recognize\"), die mit dieser Bibliothek benutzt werden können. Benutzen Sie in dieser Aufgabe Googles online (!) Modell. \n",
    "\n",
    "Nehmen Sie eine eigene Audio Datei auf und lassen Sie diese von Ihrem Programm in Text umwandeln.\n",
    "\n",
    "Bevor wir verschiedene Versuche mit unserem Programm machen, lassen Sie uns gemeinsam eine Lösungsvariante anschauen.\n",
    "\n",
    "![alt text](image-1.png)\n",
    "Nikitas Code\n",
    "![alt text](image-3.png)\n",
    "Sascha Utner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren des Moduls speech_recognition\n",
    "\n",
    "\n",
    "# Erstellen eines speech_recognition Objektes\n",
    "\n",
    "\n",
    "# Audio-Datei laden\n",
    "\n",
    "\n",
    "# Erkennung mit Googles online Modell durchführen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Aufgabe\n",
    "\n",
    "- Testen Sie verschiedene Audioaufnahmen. Wie schlecht muss die Qualität sein, damit Googles online Modell Probleme bei der Erkennung hat?\n",
    "- Testen Sie auch verschiedene Sprachen, soweit möglich :) \n",
    "- Testen Sie eine Kombination aus Audioaufnahme und Spracheinstellung im code, die nicht zusammengehören."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-2.png)\n",
    "Nikita (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online vs. Lokal\n",
    "\n",
    "Die Nutzung der Google *online* Spracherkennung ist mit Nachteilen verbunden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Aufgabe\n",
    "\n",
    "Mit dem Modul SpeechRecognition kann auch offline Spracherkennung betrieben werden.\n",
    "Installieren Sie dazu das Python Paket `pocketsphinx`. Es enthält das offline SprachModell *für englische Sprache*.\n",
    "Ändern Sie Ihr obiges Programm so ab, dass anstelle des Google online Modells nun das offline Modell *Sphinx* benutzt wird.\n",
    "\n",
    "Analysieren Sie verschiedene Audioaufnahmen/-dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren des Moduls speech_recognition\n",
    "\n",
    "\n",
    "# Erstellen eines speech_recognition Objektes\n",
    "\n",
    "\n",
    "# Audio-Datei laden\n",
    "\n",
    "\n",
    "# Offline-Erkennung mit Sphinx durchführen\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
